{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autores: \n",
    "**Gabriel Roberto (221020870) e Jean Soares (241033810)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YidpKSlo4iSk"
   },
   "source": [
    "# Noções de IA 2/24 - Trabalho 2\n",
    "## Rede Convolucional e Transfer Learning\n",
    "### 1. Introdução e Base de Dados\n",
    "Neste trabalho usaremos uma rede convolucional pré-treinada e a aplicaremos em um problema novo. Também experimentaremos com a divisão da base em treinamento, validação e teste, e usaremos validação para o \"early stopping\" na tentativa de controlar o sobre-ajuste.\n",
    "\n",
    "A base de dados é a \"TensorFlow Flowers Dataset\". Ela contém 3670 imagens coloridas de flores pertencentes a uma de  5 classes: Margarida, Dente-de-leão, Rosa, Girassol e Tulipa. [Esta página descritiva](https://www.tensorflow.org/datasets/catalog/tf_flowers) contém alguns exemplos.\n",
    "\n",
    "Ela pode ser baixada com o código abaixo.\n",
    "\n",
    "Obs: o módulo tensorflow_datasets a princípio não é acessível em instalações Windows. Você pode usar uma instalação local Unix, ou um serviço de núvem como o Google Colab ou Amazon Web Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "e05a9944195344089f7183ee778647f1",
      "3957bf475e014679908333c7713434dc",
      "09f9784119a348918fc4810efbfcfad1",
      "1523cd2b26a940a292d7c881ef517a24",
      "06c55149ad134ed9b91e2468401e14a9",
      "ae5592435fc840428e7083c8ba1da18c",
      "e66f0d9ff6004c0ebad44b1557b2fcfd",
      "c7395c974ab44e90bf8014ddb9590323",
      "cbaced8919b143a2816487b0ae4a663c",
      "e4d8ae728d214d9ab6de584bcf44bceb",
      "deeb00f1d2ce47d098e2bdeadfff2ff9"
     ]
    },
    "collapsed": true,
    "id": "Ef2uTTEXumSC",
    "outputId": "3d257218-dd18-424b-ab5b-d469fea10275"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 23:36:48.046777: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/coffee/miniconda3/envs/VGG/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-21 23:37:04.551155: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-21 23:37:04.571184: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2569, 150, 150, 3)\n",
      "(1101, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "## Loading images and labels\n",
    "(train_ds, train_labels), (test_ds, test_labels) = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:70%]\", \"train[70%:]\"], ## Train test split\n",
    "    batch_size=-1,\n",
    "    as_supervised=True,  # Include labels\n",
    ")\n",
    "\n",
    "## Resizing images\n",
    "train_ds = tf.image.resize(train_ds, (150, 150))\n",
    "test_ds = tf.image.resize(test_ds, (150, 150))\n",
    "\n",
    "## Transforming labels to correct format\n",
    "train_labels = to_categorical(train_labels, num_classes=5)\n",
    "test_labels = to_categorical(test_labels, num_classes=5)\n",
    "print (train_ds.shape)\n",
    "print (test_ds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiUqibAC8Iji"
   },
   "source": [
    "Observe como foi feita a separação em 70% de dados para treinamento e 30% de dados para teste. Observe também que a base de dados original tem apenas um conjunto \"train\". A separação em treinamento, teste e validação é feita pelo usuário. Por isso a instrução para obter 70% do conjunto \"train\" e 30% do conjunto \"train\", o que soa estranho a princípio.\n",
    "\n",
    "### 2. Treinando um MLP\n",
    "\n",
    "Use esta base de dados para treinar um MLP, como feito no trabalho anterior com a base MNIST.\n",
    "\n",
    "Escolha um MLP com 2 camadas escondidas. Não perca muito tempo variando a arquitetura porque este problema é difícil sem o uso de convoluções e o resultado não será totalmente satisfatório.\n",
    "\n",
    "Você pode usar este código como base para definição da rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OX8eq2WY_AUm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(1024, activation='relu')\n",
    "dense_layer_2 = layers.Dense(512, activation='relu')\n",
    "prediction_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7WTR_sX_HZ0"
   },
   "source": [
    "Observe que as imagens são achatadas (transformadas em vetor). Substitua as interrogações pelo tamanho desejado das camadas escondidas.\n",
    "\n",
    "Neste problema vamos verificar o fenômeno do \"over-fitting\", e vamos tentar equilibrá-lo pela técnica de parada prematura de treinamento. Assim, dos dados de treinamento precisamos fazer uma nova separação para validação. Quando a acurácia de validação não sobe num dado número de épocas (o parâmetro \"patience\"), o treinamento é interrompido. Este trecho de código pode ser útil:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YUSaZJ4EASRM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 42s 624ms/step - loss: 759.5353 - accuracy: 0.2803 - val_loss: 143.4751 - val_accuracy: 0.2704\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 105.1049 - accuracy: 0.3659 - val_loss: 89.5925 - val_accuracy: 0.3307\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 38s 580ms/step - loss: 46.1946 - accuracy: 0.4112 - val_loss: 36.1674 - val_accuracy: 0.3833\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 37s 572ms/step - loss: 22.3412 - accuracy: 0.4365 - val_loss: 25.4227 - val_accuracy: 0.3502\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 14.0775 - accuracy: 0.4749 - val_loss: 15.5660 - val_accuracy: 0.4514\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 26s 402ms/step - loss: 10.3654 - accuracy: 0.4973 - val_loss: 14.6379 - val_accuracy: 0.4261\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 28s 439ms/step - loss: 8.0994 - accuracy: 0.5241 - val_loss: 17.2516 - val_accuracy: 0.3249\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 27s 412ms/step - loss: 7.6718 - accuracy: 0.5275 - val_loss: 15.9378 - val_accuracy: 0.3872\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 28s 427ms/step - loss: 6.4549 - accuracy: 0.5426 - val_loss: 13.6661 - val_accuracy: 0.4280\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 26s 405ms/step - loss: 5.9508 - accuracy: 0.5596 - val_loss: 9.4843 - val_accuracy: 0.4202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75b0159d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
    "\n",
    "model.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j6ojtE6AZ85"
   },
   "source": [
    "Os parâmetros dados são sugestões.\n",
    "Você agora pode testar o seu modelo, por exemplo, com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QNuvd0fGBKrO",
    "outputId": "72756349-9123-438e-f3f0-0378e093d207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 5s 129ms/step - loss: 831.2014 - accuracy: 0.3569\n",
      "Accuracy: 0.35694822669029236\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(test_ds, test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ6JtiX8BJBj"
   },
   "source": [
    "Mais uma vez, procure realizar ajustes, mas não espere um bom desempenho. Como dissemos, é um problema complexo de classificação de imagem, e é difícil fazer o MLP funcionar sozinho. Precisamos de um pré-processamento com base em uma rede convolucional.\n",
    "\n",
    "### 3. Uso da rede VGG16 pré-treinada.\n",
    "\n",
    "Lembre-se que a rede VGG usa como bloco básico uma cascata de convoluções com filtros 3x3, com \"padding\" para que a imagem não seja diminuída, seguida de um \"max pooling\" reduzindo imagens pela metade. O número de mapas vai aumentando e seu tamanho vai diminuindo ao longo de suas 16 camadas. Este é um modelo gigantesco e o treinamento com recursos computacionais modestos levaria dias ou semanas, se é que fosse possível.\n",
    "\n",
    "No entanto, vamos aproveitar uma característica central das grandes redes convolucionais. Elas podem ser usadas como pré-processamento fixo das imagens, mesmo em um novo problema. (Lembre-se, a rede VGG original foi treinada na base ImageNet, que tem muitas categorias de imagem, não apenas flores).\n",
    "\n",
    "O código abaixo realiza o download do modelo treinado, especifica que não será usada a última camada, e que os parâmetros do modelo-base não são ajustáveis.\n",
    "\n",
    "Observe que a classe também tem o método preprocess_input para garantir que os dados sejam processados de maneira semelhante ao treinamento original da VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WP-b_vl6wH0n",
    "outputId": "2958e327-2dd2-4785-a326-40792b18191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "## Preprocessing input\n",
    "train_ds = preprocess_input(train_ds)\n",
    "test_ds = preprocess_input(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEQ-rCzmEtP8"
   },
   "source": [
    "Você pode checar a arquitetura do modelo-base com o método \"summary\". Observe que há incríveis 14 milhões de parâmetros no modelo, que felizmente não vamos precisar adaptar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-J8S_BLpyHrS",
    "outputId": "3ee85e16-fb1e-4b0d-e2e2-740a6ba9577b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_6yErtmFVtK"
   },
   "source": [
    "Agora, monte a rede final de forma semelhante ao MLP acima. Use o base_model como primeira camada, seguida de uma camada \"flatten\" (o MLP espera um vetor de entradas), e duas camadas densas. Elas não precisam ser muito grandes. Experimente com 50 e 20, respectivamente, ou algo próximo.\n",
    "\n",
    "Volte a treinar e testar o modelo. Mesmo sem efetivamente treinar a rede VGG, ainda temos que passar os dados por ela a cada passo, e o treinamento é um tanto lento. Mas desta vez o problema deve ser resolvido satisfatoriamente.\n",
    "\n",
    "### 4. Extras (opcionais)\n",
    "\n",
    "4.1. Procure usar outra(s) redes convolucionais como base.\n",
    "\n",
    "4.2. No lugar de \"early stopping\", experimente com regularização L1 e L2, e \"dropout\".\n",
    "\n",
    "4.3. Procure (ou produza) imagens de algumas flores destas categorias, e teste em seu modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lW3PO5y7BPyl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 262s 4s/step - loss: 1.7642 - accuracy: 0.4209 - val_loss: 1.2325 - val_accuracy: 0.4844\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 298s 5s/step - loss: 0.9484 - accuracy: 0.6161 - val_loss: 1.1139 - val_accuracy: 0.5778\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 263s 4s/step - loss: 0.7380 - accuracy: 0.7095 - val_loss: 1.1430 - val_accuracy: 0.5973\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 247s 4s/step - loss: 0.5588 - accuracy: 0.7796 - val_loss: 1.1441 - val_accuracy: 0.6265\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 279s 4s/step - loss: 0.4588 - accuracy: 0.8243 - val_loss: 1.1234 - val_accuracy: 0.6284\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 260s 4s/step - loss: 0.3705 - accuracy: 0.8535 - val_loss: 1.2276 - val_accuracy: 0.6576\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 258s 4s/step - loss: 0.2876 - accuracy: 0.8842 - val_loss: 1.2959 - val_accuracy: 0.6576\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 245s 4s/step - loss: 0.2239 - accuracy: 0.9119 - val_loss: 1.2892 - val_accuracy: 0.6615\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 244s 4s/step - loss: 0.1984 - accuracy: 0.9212 - val_loss: 1.3860 - val_accuracy: 0.6907\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 247s 4s/step - loss: 0.1704 - accuracy: 0.9304 - val_loss: 1.3890 - val_accuracy: 0.7140\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 243s 4s/step - loss: 0.1419 - accuracy: 0.9460 - val_loss: 1.3900 - val_accuracy: 0.6887\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 224s 3s/step - loss: 0.1508 - accuracy: 0.9397 - val_loss: 1.3474 - val_accuracy: 0.7023\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 209s 3s/step - loss: 0.1054 - accuracy: 0.9601 - val_loss: 1.4947 - val_accuracy: 0.6887\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 201s 3s/step - loss: 0.0761 - accuracy: 0.9674 - val_loss: 1.5349 - val_accuracy: 0.7082\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 195s 3s/step - loss: 0.0630 - accuracy: 0.9800 - val_loss: 1.6231 - val_accuracy: 0.7004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75783d4400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(50, activation='relu')\n",
    "dense_layer_2 = layers.Dense(20, activation='relu')\n",
    "prediction_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
    "\n",
    "model.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 93s 3s/step - loss: 1.9028 - accuracy: 0.6494\n",
      "Accuracy: 0.6494096517562866\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(test_ds, test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "## Loading VGG16 model\n",
    "base_model_41_vgg19 = VGG19(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)\n",
    "base_model_41_vgg19.trainable = False ## Not\n",
    "\n",
    "## Preprocessing input\n",
    "train_ds_41_vgg19 = preprocess_input(train_ds)\n",
    "test_ds_41_vgg19 = preprocess_input(test_ds)\n",
    "\n",
    "base_model_41_vgg19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 305s 5s/step - loss: 1.1286 - accuracy: 0.5961 - val_loss: 1.1963 - val_accuracy: 0.5623\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 367s 6s/step - loss: 0.7875 - accuracy: 0.6793 - val_loss: 1.2279 - val_accuracy: 0.6459\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 311s 5s/step - loss: 0.6449 - accuracy: 0.7377 - val_loss: 1.2064 - val_accuracy: 0.6284\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 315s 5s/step - loss: 0.5161 - accuracy: 0.7771 - val_loss: 1.2218 - val_accuracy: 0.6498\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 377s 6s/step - loss: 0.4526 - accuracy: 0.8170 - val_loss: 1.2837 - val_accuracy: 0.6498\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 298s 5s/step - loss: 0.3748 - accuracy: 0.8482 - val_loss: 1.3996 - val_accuracy: 0.6537\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 261s 4s/step - loss: 0.3248 - accuracy: 0.8637 - val_loss: 1.5174 - val_accuracy: 0.6518\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 244s 4s/step - loss: 0.3143 - accuracy: 0.8715 - val_loss: 1.4564 - val_accuracy: 0.6654\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 244s 4s/step - loss: 0.2695 - accuracy: 0.8856 - val_loss: 1.5800 - val_accuracy: 0.6498\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 241s 4s/step - loss: 0.2256 - accuracy: 0.9051 - val_loss: 1.7170 - val_accuracy: 0.6576\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 242s 4s/step - loss: 0.1862 - accuracy: 0.9182 - val_loss: 1.7328 - val_accuracy: 0.6595\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 242s 4s/step - loss: 0.1753 - accuracy: 0.9309 - val_loss: 2.0582 - val_accuracy: 0.6498\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 241s 4s/step - loss: 0.1509 - accuracy: 0.9377 - val_loss: 1.9813 - val_accuracy: 0.6556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74ba34d310>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_41_vgg19 = models.Sequential([\n",
    "    base_model_41_vgg19,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model_41_vgg19.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model_41_vgg19.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 101s 3s/step - loss: 2.9957 - accuracy: 0.6113\n",
      "Accuracy: 0.6112625002861023\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss_41_vgg19, accuracy_41_vgg19 = model_41_vgg19.evaluate(test_ds, test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy_41_vgg19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "65/65 [==============================] - 209s 3s/step - loss: 0.9116 - accuracy: 0.6779 - val_loss: 1.3287 - val_accuracy: 0.6012\n",
      "Epoch 2/15\n",
      "65/65 [==============================] - 205s 3s/step - loss: 0.9537 - accuracy: 0.6662 - val_loss: 1.4791 - val_accuracy: 0.5759\n",
      "Epoch 3/15\n",
      "65/65 [==============================] - 214s 3s/step - loss: 0.7971 - accuracy: 0.7027 - val_loss: 1.6480 - val_accuracy: 0.6206\n",
      "Epoch 4/15\n",
      "65/65 [==============================] - 335s 5s/step - loss: 0.8487 - accuracy: 0.6920 - val_loss: 1.2882 - val_accuracy: 0.6381\n",
      "Epoch 5/15\n",
      "65/65 [==============================] - 353s 5s/step - loss: 0.7406 - accuracy: 0.7270 - val_loss: 1.4705 - val_accuracy: 0.6265\n",
      "Epoch 6/15\n",
      "65/65 [==============================] - 341s 5s/step - loss: 0.7060 - accuracy: 0.7377 - val_loss: 1.3749 - val_accuracy: 0.6440\n",
      "Epoch 7/15\n",
      "65/65 [==============================] - 258s 4s/step - loss: 0.7308 - accuracy: 0.7231 - val_loss: 1.4457 - val_accuracy: 0.6556\n",
      "Epoch 8/15\n",
      "65/65 [==============================] - 206s 3s/step - loss: 0.6762 - accuracy: 0.7474 - val_loss: 1.3150 - val_accuracy: 0.6440\n",
      "Epoch 9/15\n",
      "65/65 [==============================] - 225s 3s/step - loss: 0.6474 - accuracy: 0.7752 - val_loss: 1.3627 - val_accuracy: 0.6401\n",
      "Epoch 10/15\n",
      "65/65 [==============================] - 209s 3s/step - loss: 0.6088 - accuracy: 0.7674 - val_loss: 1.3893 - val_accuracy: 0.6459\n",
      "Epoch 11/15\n",
      "65/65 [==============================] - 207s 3s/step - loss: 0.5755 - accuracy: 0.7796 - val_loss: 1.3380 - val_accuracy: 0.6673\n",
      "Epoch 12/15\n",
      "65/65 [==============================] - 236s 4s/step - loss: 0.5615 - accuracy: 0.7898 - val_loss: 1.3833 - val_accuracy: 0.6459\n",
      "Epoch 13/15\n",
      "65/65 [==============================] - 245s 4s/step - loss: 0.6354 - accuracy: 0.7854 - val_loss: 1.2510 - val_accuracy: 0.6556\n",
      "Epoch 14/15\n",
      "65/65 [==============================] - 234s 4s/step - loss: 0.5196 - accuracy: 0.7961 - val_loss: 1.4299 - val_accuracy: 0.6459\n",
      "Epoch 15/15\n",
      "65/65 [==============================] - 217s 3s/step - loss: 0.5340 - accuracy: 0.7995 - val_loss: 1.4208 - val_accuracy: 0.6712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74ac522220>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model42 = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    Dropout(0.2),\n",
    "    dense_layer_1,\n",
    "    Dropout(0.2),\n",
    "    dense_layer_2,\n",
    "    Dropout(0.2),\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model42.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model42.fit(train_ds, train_labels, epochs=15, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 95s 3s/step - loss: 2.2183 - accuracy: 0.6549\n",
      "Accuracy: 0.6548592448234558\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss_42_drop, accuracy_42_drop = model42.evaluate(test_ds, test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy_42_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 e L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "65/65 [==============================] - 226s 3s/step - loss: 22.3862 - accuracy: 0.4720 - val_loss: 9.6135 - val_accuracy: 0.5837\n",
      "Epoch 2/15\n",
      "65/65 [==============================] - 223s 3s/step - loss: 6.7768 - accuracy: 0.6725 - val_loss: 5.3639 - val_accuracy: 0.6167\n",
      "Epoch 3/15\n",
      "65/65 [==============================] - 250s 4s/step - loss: 4.5000 - accuracy: 0.7109 - val_loss: 4.2519 - val_accuracy: 0.6265\n",
      "Epoch 4/15\n",
      "65/65 [==============================] - 189s 3s/step - loss: 3.6016 - accuracy: 0.7246 - val_loss: 3.7660 - val_accuracy: 0.5837\n",
      "Epoch 5/15\n",
      "65/65 [==============================] - 184s 3s/step - loss: 3.3336 - accuracy: 0.7168 - val_loss: 3.4451 - val_accuracy: 0.6109\n",
      "Epoch 6/15\n",
      "65/65 [==============================] - 186s 3s/step - loss: 2.9325 - accuracy: 0.7397 - val_loss: 2.9951 - val_accuracy: 0.6148\n",
      "Epoch 7/15\n",
      "65/65 [==============================] - 184s 3s/step - loss: 2.7346 - accuracy: 0.7489 - val_loss: 3.0743 - val_accuracy: 0.6206\n",
      "Epoch 8/15\n",
      "65/65 [==============================] - 181s 3s/step - loss: 2.6482 - accuracy: 0.7353 - val_loss: 2.9784 - val_accuracy: 0.5856\n",
      "Epoch 9/15\n",
      "65/65 [==============================] - 171s 3s/step - loss: 2.4009 - accuracy: 0.7547 - val_loss: 2.6727 - val_accuracy: 0.6342\n",
      "Epoch 10/15\n",
      "65/65 [==============================] - 203s 3s/step - loss: 2.3648 - accuracy: 0.7401 - val_loss: 2.9126 - val_accuracy: 0.5973\n",
      "Epoch 11/15\n",
      "65/65 [==============================] - 215s 3s/step - loss: 2.2651 - accuracy: 0.7586 - val_loss: 2.5157 - val_accuracy: 0.6634\n",
      "Epoch 12/15\n",
      "65/65 [==============================] - 218s 3s/step - loss: 2.2208 - accuracy: 0.7538 - val_loss: 2.5876 - val_accuracy: 0.6342\n",
      "Epoch 13/15\n",
      "65/65 [==============================] - 217s 3s/step - loss: 2.0742 - accuracy: 0.7762 - val_loss: 2.4648 - val_accuracy: 0.6304\n",
      "Epoch 14/15\n",
      "65/65 [==============================] - 216s 3s/step - loss: 2.0084 - accuracy: 0.7791 - val_loss: 2.5487 - val_accuracy: 0.6576\n",
      "Epoch 15/15\n",
      "65/65 [==============================] - 218s 3s/step - loss: 2.1682 - accuracy: 0.7523 - val_loss: 2.4074 - val_accuracy: 0.6479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74ba0f0850>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dense_layer_1_L1L2 = layers.Dense(50, activation='relu', kernel_regularizer='l1_l2')\n",
    "dense_layer_2_L1L2 = layers.Dense(20, activation='relu', kernel_regularizer='l1_l2')\n",
    "\n",
    "model_L1L2 = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1_L1L2,\n",
    "    dense_layer_2_L1L2,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model_L1L2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model_L1L2.fit(train_ds, train_labels, epochs=15, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 100s 3s/step - loss: 2.5088 - accuracy: 0.5804\n",
      "Accuracy: 0.580381453037262\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss_42_L1L12, accuracy_42_L1L12 = model_L1L2.evaluate(test_ds, test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy_42_L1L12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m   img_array \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Create a batch\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#   predictions = model.predict(img_array)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m   score \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mpredictions\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     33\u001b[0m   plt\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[1;32m     34\u001b[0m   plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def text_labels(indice):\n",
    "    labels = ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']\n",
    "    return labels[int(indice)]\n",
    "\n",
    "urls_imagens_teste = [\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\",\n",
    "    \"https://th.bing.com/th/id/OIP.NqainmbKbnBKtyo2Iv-xmwHaFj?rs=1&pid=ImgDetMain\",\n",
    "    \"https://i.pinimg.com/originals/8a/e7/cb/8ae7cb19acd814a0e73de5e329a11753.jpg\",\n",
    "    \"https://th.bing.com/th/id/R.97e56aade3511220571befa78547260c?rik=dUCjh4ER35JIsA&riu=http%3a%2f%2fblog.giulianaflores.com.br%2fwp-content%2fuploads%2f2013%2f11%2fcultivar-margaridas.jpg&ehk=lV3vBT8eLOtyVHzvucdVJy9ev8Py12qTrC0R6WxkXfk%3d&risl=&pid=ImgRaw&r=0\",\n",
    "    \"https://http2.mlstatic.com/rosa-pink-rose-arbustiva-sementes-flor-para-mudas-D_NQ_NP_7746-MLB5276609140_102013-F.jpg\"\n",
    "]\n",
    "\n",
    "labels = ['sunflowers', 'tulips', 'dandelion', 'daisy', 'roses']\n",
    "\n",
    "i = 0\n",
    "for imagem in urls_imagens_teste:\n",
    "  flower_path = tf.keras.utils.get_file(labels[i], origin=imagem)\n",
    "\n",
    "  img = tf.keras.preprocessing.image.load_img(\n",
    "        flower_path, target_size=(150, 150)\n",
    "    )\n",
    "  \n",
    "  img_array = tf.keras.utils.img_to_array(img)\n",
    "  img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "  score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "  plt.imshow(img)\n",
    "  plt.show()\n",
    "\n",
    "  print(\n",
    "      \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "      .format(text_labels(np.argmax(score)), 100 * np.max(score))\n",
    "  )\n",
    "\n",
    "  print(\n",
    "      f\"Real: {labels[i]}\"\n",
    "  )\n",
    "\n",
    "  i += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "VGG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06c55149ad134ed9b91e2468401e14a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09f9784119a348918fc4810efbfcfad1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7395c974ab44e90bf8014ddb9590323",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cbaced8919b143a2816487b0ae4a663c",
      "value": 5
     }
    },
    "1523cd2b26a940a292d7c881ef517a24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4d8ae728d214d9ab6de584bcf44bceb",
      "placeholder": "​",
      "style": "IPY_MODEL_deeb00f1d2ce47d098e2bdeadfff2ff9",
      "value": " 5/5 [00:02&lt;00:00,  2.07 file/s]"
     }
    },
    "3957bf475e014679908333c7713434dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae5592435fc840428e7083c8ba1da18c",
      "placeholder": "​",
      "style": "IPY_MODEL_e66f0d9ff6004c0ebad44b1557b2fcfd",
      "value": "Dl Completed...: 100%"
     }
    },
    "ae5592435fc840428e7083c8ba1da18c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7395c974ab44e90bf8014ddb9590323": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbaced8919b143a2816487b0ae4a663c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "deeb00f1d2ce47d098e2bdeadfff2ff9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e05a9944195344089f7183ee778647f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3957bf475e014679908333c7713434dc",
       "IPY_MODEL_09f9784119a348918fc4810efbfcfad1",
       "IPY_MODEL_1523cd2b26a940a292d7c881ef517a24"
      ],
      "layout": "IPY_MODEL_06c55149ad134ed9b91e2468401e14a9"
     }
    },
    "e4d8ae728d214d9ab6de584bcf44bceb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e66f0d9ff6004c0ebad44b1557b2fcfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
