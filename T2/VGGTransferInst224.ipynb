{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YidpKSlo4iSk"
   },
   "source": [
    "# Noções de IA 2/24 - Trabalho 2\n",
    "## Rede Convolucional e Transfer Learning\n",
    "### 1. Introdução e Base de Dados\n",
    "Neste trabalho usaremos uma rede convolucional pré-treinada e a aplicaremos em um problema novo. Também experimentaremos com a divisão da base em treinamento, validação e teste, e usaremos validação para o \"early stopping\" na tentativa de controlar o sobre-ajuste.\n",
    "\n",
    "A base de dados é a \"TensorFlow Flowers Dataset\". Ela contém 3670 imagens coloridas de flores pertencentes a uma de  5 classes: Margarida, Dente-de-leão, Rosa, Girassol e Tulipa. [Esta página descritiva](https://www.tensorflow.org/datasets/catalog/tf_flowers) contém alguns exemplos.\n",
    "\n",
    "Ela pode ser baixada com o código abaixo.\n",
    "\n",
    "Obs: o módulo tensorflow_datasets a princípio não é acessível em instalações Windows. Você pode usar uma instalação local Unix, ou um serviço de núvem como o Google Colab ou Amazon Web Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "e05a9944195344089f7183ee778647f1",
      "3957bf475e014679908333c7713434dc",
      "09f9784119a348918fc4810efbfcfad1",
      "1523cd2b26a940a292d7c881ef517a24",
      "06c55149ad134ed9b91e2468401e14a9",
      "ae5592435fc840428e7083c8ba1da18c",
      "e66f0d9ff6004c0ebad44b1557b2fcfd",
      "c7395c974ab44e90bf8014ddb9590323",
      "cbaced8919b143a2816487b0ae4a663c",
      "e4d8ae728d214d9ab6de584bcf44bceb",
      "deeb00f1d2ce47d098e2bdeadfff2ff9"
     ]
    },
    "collapsed": true,
    "id": "Ef2uTTEXumSC",
    "outputId": "3d257218-dd18-424b-ab5b-d469fea10275"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 23:36:48.046777: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/coffee/miniconda3/envs/VGG/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-21 23:37:04.551155: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-21 23:37:04.571184: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2569, 150, 150, 3)\n",
      "(1101, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "## Loading images and labels\n",
    "(train_ds, train_labels), (test_ds, test_labels) = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:70%]\", \"train[70%:]\"], ## Train test split\n",
    "    batch_size=-1,\n",
    "    as_supervised=True,  # Include labels\n",
    ")\n",
    "\n",
    "## Resizing images\n",
    "train_ds = tf.image.resize(train_ds, (150, 150))\n",
    "test_ds = tf.image.resize(test_ds, (150, 150))\n",
    "\n",
    "## Transforming labels to correct format\n",
    "train_labels = to_categorical(train_labels, num_classes=5)\n",
    "test_labels = to_categorical(test_labels, num_classes=5)\n",
    "print (train_ds.shape)\n",
    "print (test_ds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiUqibAC8Iji"
   },
   "source": [
    "Observe como foi feita a separação em 70% de dados para treinamento e 30% de dados para teste. Observe também que a base de dados original tem apenas um conjunto \"train\". A separação em treinamento, teste e validação é feita pelo usuário. Por isso a instrução para obter 70% do conjunto \"train\" e 30% do conjunto \"train\", o que soa estranho a princípio.\n",
    "\n",
    "### 2. Treinando um MLP\n",
    "\n",
    "Use esta base de dados para treinar um MLP, como feito no trabalho anterior com a base MNIST.\n",
    "\n",
    "Escolha um MLP com 2 camadas escondidas. Não perca muito tempo variando a arquitetura porque este problema é difícil sem o uso de convoluções e o resultado não será totalmente satisfatório.\n",
    "\n",
    "Você pode usar este código como base para definição da rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OX8eq2WY_AUm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(1024, activation='relu')\n",
    "dense_layer_2 = layers.Dense(512, activation='relu')\n",
    "prediction_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7WTR_sX_HZ0"
   },
   "source": [
    "Observe que as imagens são achatadas (transformadas em vetor). Substitua as interrogações pelo tamanho desejado das camadas escondidas.\n",
    "\n",
    "Neste problema vamos verificar o fenômeno do \"over-fitting\", e vamos tentar equilibrá-lo pela técnica de parada prematura de treinamento. Assim, dos dados de treinamento precisamos fazer uma nova separação para validação. Quando a acurácia de validação não sobe num dado número de épocas (o parâmetro \"patience\"), o treinamento é interrompido. Este trecho de código pode ser útil:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YUSaZJ4EASRM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 42s 624ms/step - loss: 759.5353 - accuracy: 0.2803 - val_loss: 143.4751 - val_accuracy: 0.2704\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 105.1049 - accuracy: 0.3659 - val_loss: 89.5925 - val_accuracy: 0.3307\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 38s 580ms/step - loss: 46.1946 - accuracy: 0.4112 - val_loss: 36.1674 - val_accuracy: 0.3833\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 37s 572ms/step - loss: 22.3412 - accuracy: 0.4365 - val_loss: 25.4227 - val_accuracy: 0.3502\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 14.0775 - accuracy: 0.4749 - val_loss: 15.5660 - val_accuracy: 0.4514\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 26s 402ms/step - loss: 10.3654 - accuracy: 0.4973 - val_loss: 14.6379 - val_accuracy: 0.4261\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 28s 439ms/step - loss: 8.0994 - accuracy: 0.5241 - val_loss: 17.2516 - val_accuracy: 0.3249\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 27s 412ms/step - loss: 7.6718 - accuracy: 0.5275 - val_loss: 15.9378 - val_accuracy: 0.3872\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 28s 427ms/step - loss: 6.4549 - accuracy: 0.5426 - val_loss: 13.6661 - val_accuracy: 0.4280\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 26s 405ms/step - loss: 5.9508 - accuracy: 0.5596 - val_loss: 9.4843 - val_accuracy: 0.4202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75b0159d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
    "\n",
    "model.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j6ojtE6AZ85"
   },
   "source": [
    "Os parâmetros dados são sugestões.\n",
    "Você agora pode testar o seu modelo, por exemplo, com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QNuvd0fGBKrO",
    "outputId": "72756349-9123-438e-f3f0-0378e093d207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 5s 129ms/step - loss: 831.2014 - accuracy: 0.3569\n",
      "Accuracy: 0.35694822669029236\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(test_ds, test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ6JtiX8BJBj"
   },
   "source": [
    "Mais uma vez, procure realizar ajustes, mas não espere um bom desempenho. Como dissemos, é um problema complexo de classificação de imagem, e é difícil fazer o MLP funcionar sozinho. Precisamos de um pré-processamento com base em uma rede convolucional.\n",
    "\n",
    "### 3. Uso da rede VGG16 pré-treinada.\n",
    "\n",
    "Lembre-se que a rede VGG usa como bloco básico uma cascata de convoluções com filtros 3x3, com \"padding\" para que a imagem não seja diminuída, seguida de um \"max pooling\" reduzindo imagens pela metade. O número de mapas vai aumentando e seu tamanho vai diminuindo ao longo de suas 16 camadas. Este é um modelo gigantesco e o treinamento com recursos computacionais modestos levaria dias ou semanas, se é que fosse possível.\n",
    "\n",
    "No entanto, vamos aproveitar uma característica central das grandes redes convolucionais. Elas podem ser usadas como pré-processamento fixo das imagens, mesmo em um novo problema. (Lembre-se, a rede VGG original foi treinada na base ImageNet, que tem muitas categorias de imagem, não apenas flores).\n",
    "\n",
    "O código abaixo realiza o download do modelo treinado, especifica que não será usada a última camada, e que os parâmetros do modelo-base não são ajustáveis.\n",
    "\n",
    "Observe que a classe também tem o método preprocess_input para garantir que os dados sejam processados de maneira semelhante ao treinamento original da VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WP-b_vl6wH0n",
    "outputId": "2958e327-2dd2-4785-a326-40792b18191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "## Preprocessing input\n",
    "train_ds = preprocess_input(train_ds)\n",
    "test_ds = preprocess_input(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEQ-rCzmEtP8"
   },
   "source": [
    "Você pode checar a arquitetura do modelo-base com o método \"summary\". Observe que há incríveis 14 milhões de parâmetros no modelo, que felizmente não vamos precisar adaptar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-J8S_BLpyHrS",
    "outputId": "3ee85e16-fb1e-4b0d-e2e2-740a6ba9577b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_6yErtmFVtK"
   },
   "source": [
    "Agora, monte a rede final de forma semelhante ao MLP acima. Use o base_model como primeira camada, seguida de uma camada \"flatten\" (o MLP espera um vetor de entradas), e duas camadas densas. Elas não precisam ser muito grandes. Experimente com 50 e 20, respectivamente, ou algo próximo.\n",
    "\n",
    "Volte a treinar e testar o modelo. Mesmo sem efetivamente treinar a rede VGG, ainda temos que passar os dados por ela a cada passo, e o treinamento é um tanto lento. Mas desta vez o problema deve ser resolvido satisfatoriamente.\n",
    "\n",
    "### 4. Extras (opcionais)\n",
    "\n",
    "4.1. Procure usar outra(s) redes convolucionais como base.\n",
    "\n",
    "4.2. No lugar de \"early stopping\", experimente com regularização L1 e L2, e \"dropout\".\n",
    "\n",
    "4.3. Procure (ou produza) imagens de algumas flores destas categorias, e teste em seu modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lW3PO5y7BPyl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 262s 4s/step - loss: 1.7642 - accuracy: 0.4209 - val_loss: 1.2325 - val_accuracy: 0.4844\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 298s 5s/step - loss: 0.9484 - accuracy: 0.6161 - val_loss: 1.1139 - val_accuracy: 0.5778\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 263s 4s/step - loss: 0.7380 - accuracy: 0.7095 - val_loss: 1.1430 - val_accuracy: 0.5973\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 247s 4s/step - loss: 0.5588 - accuracy: 0.7796 - val_loss: 1.1441 - val_accuracy: 0.6265\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 279s 4s/step - loss: 0.4588 - accuracy: 0.8243 - val_loss: 1.1234 - val_accuracy: 0.6284\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 260s 4s/step - loss: 0.3705 - accuracy: 0.8535 - val_loss: 1.2276 - val_accuracy: 0.6576\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 258s 4s/step - loss: 0.2876 - accuracy: 0.8842 - val_loss: 1.2959 - val_accuracy: 0.6576\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 245s 4s/step - loss: 0.2239 - accuracy: 0.9119 - val_loss: 1.2892 - val_accuracy: 0.6615\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 244s 4s/step - loss: 0.1984 - accuracy: 0.9212 - val_loss: 1.3860 - val_accuracy: 0.6907\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 247s 4s/step - loss: 0.1704 - accuracy: 0.9304 - val_loss: 1.3890 - val_accuracy: 0.7140\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 243s 4s/step - loss: 0.1419 - accuracy: 0.9460 - val_loss: 1.3900 - val_accuracy: 0.6887\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 224s 3s/step - loss: 0.1508 - accuracy: 0.9397 - val_loss: 1.3474 - val_accuracy: 0.7023\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 209s 3s/step - loss: 0.1054 - accuracy: 0.9601 - val_loss: 1.4947 - val_accuracy: 0.6887\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 201s 3s/step - loss: 0.0761 - accuracy: 0.9674 - val_loss: 1.5349 - val_accuracy: 0.7082\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 195s 3s/step - loss: 0.0630 - accuracy: 0.9800 - val_loss: 1.6231 - val_accuracy: 0.7004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75783d4400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(50, activation='relu')\n",
    "dense_layer_2 = layers.Dense(20, activation='relu')\n",
    "prediction_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
    "\n",
    "model.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "VGG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06c55149ad134ed9b91e2468401e14a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09f9784119a348918fc4810efbfcfad1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7395c974ab44e90bf8014ddb9590323",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cbaced8919b143a2816487b0ae4a663c",
      "value": 5
     }
    },
    "1523cd2b26a940a292d7c881ef517a24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4d8ae728d214d9ab6de584bcf44bceb",
      "placeholder": "​",
      "style": "IPY_MODEL_deeb00f1d2ce47d098e2bdeadfff2ff9",
      "value": " 5/5 [00:02&lt;00:00,  2.07 file/s]"
     }
    },
    "3957bf475e014679908333c7713434dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae5592435fc840428e7083c8ba1da18c",
      "placeholder": "​",
      "style": "IPY_MODEL_e66f0d9ff6004c0ebad44b1557b2fcfd",
      "value": "Dl Completed...: 100%"
     }
    },
    "ae5592435fc840428e7083c8ba1da18c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7395c974ab44e90bf8014ddb9590323": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbaced8919b143a2816487b0ae4a663c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "deeb00f1d2ce47d098e2bdeadfff2ff9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e05a9944195344089f7183ee778647f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3957bf475e014679908333c7713434dc",
       "IPY_MODEL_09f9784119a348918fc4810efbfcfad1",
       "IPY_MODEL_1523cd2b26a940a292d7c881ef517a24"
      ],
      "layout": "IPY_MODEL_06c55149ad134ed9b91e2468401e14a9"
     }
    },
    "e4d8ae728d214d9ab6de584bcf44bceb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e66f0d9ff6004c0ebad44b1557b2fcfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
